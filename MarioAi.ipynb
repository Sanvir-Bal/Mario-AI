{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNstQbV0__cX"
   },
   "source": [
    "## Installations & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "executionInfo": {
     "elapsed": 15774,
     "status": "ok",
     "timestamp": 1603853036176,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "_1NdXf43AUfY",
    "outputId": "ae4a2b61-0f4f-4b47-9a6e-27a2dcbe9cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.3.0 in c:\\python\\python310\\lib\\site-packages (7.3.0)\n",
      "Requirement already satisfied: opencv-python in c:\\python\\python310\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: nes-py>=8.0.0 in c:\\python\\python310\\lib\\site-packages (from gym-super-mario-bros==7.3.0) (8.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\python\\python310\\lib\\site-packages (from opencv-python) (1.26.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\python\\python310\\lib\\site-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.22.0)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\python\\python310\\lib\\site-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\python\\python310\\lib\\site-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.66.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\python\\python310\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\python\\python310\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\python\\python310\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Install the Super Mario game environment\n",
    "!pip install gym-super-mario-bros==7.3.0 opencv-python\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, numpy as np, cv2\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Gym: an OpenAI tool for Reinforcement Learning\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "#NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "#Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdZlSOeNAyic"
   },
   "source": [
    "# Intialize The Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1603853043786,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "6T6Ju170A0nB",
    "outputId": "b2af7e1e-1aaa-4122-bf17-9f9d977eebcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-1-1-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "#Make the Super Mario environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "\n",
    "#Limit the possible actions to move right and move jump right\n",
    "env = JoypadSpace(env,[['right'],['right', 'A']])\n",
    "\n",
    "#Define next_state, reward, done, and info variables\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f'{next_state.shape},\\n {reward},\\n {done},\\n {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZLgGn1iBXL8"
   },
   "source": [
    "# Preprocess The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GaKjHlNfGNMC"
   },
   "outputs": [],
   "source": [
    "#Create class to downsize the observations \n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    #Resize the obersvation\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return observation\n",
    "\n",
    "#Create class to skip intermediate frames since frames do not differ drastically. The rewards accumulated over each skipped frame are aggregated every skip-th frame\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    #Returns only every skip-th frame\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    #Sums the rewards over the skipped frames\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            #Aggregate the reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "#Apply wrappers to the enviornment\n",
    "#Make it so that only every 4th frame is observered\n",
    "env = SkipFrame(env, skip=4)\n",
    "#Gray scale the observation to reduce size of observation\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "#Resize the obsersavtion to be 84 x 84 instead of 240 x 256\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "#Make it so that 4 consecutive frames are transformed into a single stack that is fed to the model as one observation.\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ajnkJJgPOPH"
   },
   "source": [
    "# Create The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gu_N7QFwUllG"
   },
   "outputs": [],
   "source": [
    "#Intialize class to represent agent and the agent's functions\n",
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, load_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lap2V2KEK3hq"
   },
   "source": [
    "# Act Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LCZGt9d-MGX4"
   },
   "outputs": [],
   "source": [
    "class Mario: \n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "        #Intialize the neural network to predict the most optimal action\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device='cuda')\n",
    "\n",
    "        #Define the intial exploration rate, its rate of decay, and its minimum possible value\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        #Define the number of experiences that must ellapse since the last time the model was saved for it to be saved again\n",
    "        self.save_every = 5e5\n",
    "\n",
    "    #Function to choose an epsilon-greedy action given a state\n",
    "    def act(self, state):\n",
    "        #If the model chooses to explore\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            #Take a random action\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        #Otherwise if it chooses to exploit\n",
    "        else:\n",
    "            #Use the neural network to determine the optimal action\n",
    "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model='online')\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        #Decay the exploration rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        #Increment the step counter\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMQT_j9xPDeX"
   },
   "source": [
    "# Cache & Recall Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0BkbT1HZPKJs"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario): # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    #Function to cache an experience represented by the state, next_state, action, reward, done into in the model's memory\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "        self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "    #Function to get a batch of randomly selected experiences from the memory\n",
    "    def recall(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKYxjAfTZ2kG"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SeezIHno5cQV"
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        #If the observation was of incorrect size, raise an error\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        #Intialize Q_online\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "        #Intialize Q_target to be a copy of Q_online\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        #Freeze the Q_target parameters\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    #Function to return the value of the input variable of the model from either Q_target or Q_online\n",
    "    def forward(self, input, model):\n",
    "        if model == 'online':\n",
    "            return self.online(input)\n",
    "        elif model == 'target':\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntDlzPcUkFul"
   },
   "source": [
    "## TD Estimate & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YgwTdUHBCSNq"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    #Calculate an estimate for TD using Q_online\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action]\n",
    "        return current_Q\n",
    "\n",
    "    #no_grad() used since no need to backpropagate on td_target\n",
    "    @torch.no_grad()\n",
    "    #Calculate the aggregate of the estimated TD in the next state and the current reward\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model='online')\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD0m7QwTIM9m"
   },
   "source": [
    "# Update & Sync Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8F4SxSbFYwLq"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    #Function to update Q_online based on estimate for TD and aggregate of TD estiamte and current reward\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    #Function to sync Q_target with Q_online by having Q_target's parameters become a copy of Q_online's parameters\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg6vnJFJqjIP"
   },
   "source": [
    "# Save Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XybP86dNqigo"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    #Function to save the model at a certain checkpoint\n",
    "    def save(self):\n",
    "        #Define the path the model will be saved too\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.ckpt\"\n",
    "        #Save the model\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        #Output where the model was saved to\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    #Function to load a previously trained model at a give path denoted by load_path\n",
    "    def load(self, load_path):\n",
    "        #If load_path doesn't exist, raise an error\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        #Load the model\n",
    "        chkp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = chkp.get('exploration_rate')\n",
    "        state_dict = chkp.get('model')\n",
    "\n",
    "        #Print that the model was loaded, where it was loaded from, and its current exploration rate\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXjgkNrYlVMD"
   },
   "source": [
    "# Learn Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OOXBZ9diIMqk"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        #Define the minimum number of experiences before the model starts training\n",
    "        self.burnin = 1e5\n",
    "        #Define the number of experiences between updates of Q_online\n",
    "        self.learn_every = 3\n",
    "        #Define the number of experiences that must ellapse since the last time the Q_target and Q_online were synced for them to be synced again\n",
    "        self.sync_every = 1e4\n",
    "        \n",
    "    #Function to have the model learn\n",
    "    def learn(self):\n",
    "        #If the right amount of experiences that have ellapsed since the last time the Q_target and Q_online were synced is a multiple of sync_every\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            #Sync Q_target & Q_online\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        #If the number experiences that have ellapsed since the last time the model was saved is a multiple of save_every\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            #Save the model\n",
    "            self.save()\n",
    "\n",
    "        #If the number of experiences is below the minimum number needed to begin learning return zero\n",
    "        if self.curr_step < self.burnin:\n",
    "            #Do not learn\n",
    "            return None, None\n",
    "\n",
    "        #If the number experiences that have ellapsed since the last time the model was saved isn't a multiple of learn_every\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            #Do not learn\n",
    "            return None, None\n",
    "\n",
    "        #Have the model recall experiences from its memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        #Evaluate the TD estiamte\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        #Evaluate the TD target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        #Calculate loss by backpropagating through Q_online \n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXn31zhJ6uM6"
   },
   "source": [
    "# Logger Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dHX6MBOB6tNX"
   },
   "outputs": [],
   "source": [
    "#Class to record metrics as the model learns and evolves\n",
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        #Define and open the paths that the metric information will be saved to\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        #Create the directories where the plots of the metrics will be saved to\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        #Define arrays of metrics where each index represents an episode\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        #Define arrays of metric averages where each index represents an episode\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        #Intialize current episode metrics\n",
    "        self.init_episode()\n",
    "\n",
    "        #Intialize time metric\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    #Function to log information about the current step\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "            \n",
    "    #Function to log information about a episode once it has been completed\n",
    "    def log_episode(self):\n",
    "        #Append the metrics of the current episode to their corresponding arrarys\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        #If the model isn't learning set the averages to be zero\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        #If the model is learning calculate the averages of the metrics\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        #Append the averages of the metrics to their corresponding arrarys\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "        #Reset the episode metrics\n",
    "        self.init_episode()\n",
    "\n",
    "    #Function to intialize a new episode\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    #Function to record information about all the ellapsed episodes\n",
    "    def record(self, episode, epsilon, step):\n",
    "        #Calculate the means of metrics\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "\n",
    "        #Append the means of the metrics to their corresponding arrarys\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        #Record the current time\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        #Output the metrics as well as the episode, step, and epsilon value when they were calculated\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "        #Update the plots of the metrics\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QSPezupjA6y"
   },
   "source": [
    "# Training & Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 12940,
     "status": "error",
     "timestamp": 1603853091513,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "UZxl49F6jCzu",
    "outputId": "e8950e87-323e-4694-9117-61a37e47064d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Loading model at trained_model.ckpt with exploration rate 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "C:\\Users\\Sanvi\\AppData\\Local\\Temp\\ipykernel_27448\\1084013414.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
      "C:\\Python\\Python310\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 417 - Epsilon 0.1 - Mean Reward 2330.0 - Mean Length 417.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 9.462 - Time 2023-11-05T21:16:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Define an action to be taken based on the current state of the enviornment\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#Have the agent perform that action\u001b[39;00m\n\u001b[0;32m     41\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mMario.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     28\u001b[0m     action_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#Otherwise if it chooses to exploit\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#Use the neural network to determine the optimal action\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[0;32m     34\u001b[0m     state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(state, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "#Define the path to the directory where the model will be saved\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "#Create the directory if it does not exist\n",
    "save_dir.mkdir(parents=True)\n",
    "#Intialize the Agent\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "#To Test Trained AI Model provide the path to it, and load the model at that path\n",
    "checkpoint = Path('trained_model.ckpt')\n",
    "mario.load(checkpoint)\n",
    "\n",
    "#Intialize the logger to log metrics about the model as it trains\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "#Define the number of episodes that you want to train the model for\n",
    "episodes = 20\n",
    "\n",
    "#Define the number of episodes that must ellapse since the last time the metrics relating to all the episodes had been logged for them to be logged again\n",
    "log_every = 20\n",
    "\n",
    "#Run the model for the defined number of episodes\n",
    "for e in range(episodes):\n",
    "\n",
    "    #Reset the enviornment\n",
    "    state = env.reset()\n",
    "\n",
    "    #Play the game\n",
    "    while True:\n",
    "        #Render the enviornment\n",
    "        #Do not do this while training, only when you want to observe a model\n",
    "        env.render()\n",
    "        \n",
    "        #Define an action to be taken based on the current state of the enviornment\n",
    "        action = mario.act(state)\n",
    "\n",
    "        #Have the agent perform that action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #Cache information about that action\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        #Have the model learn from the action\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        #Log the metrics relating to the current step\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        #Update the state\n",
    "        state = next_state\n",
    "\n",
    "        #Stop playing if the game is done\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    #Log the metrics relating to the episode that just finished\n",
    "    logger.log_episode()\n",
    "\n",
    "    #If log_every episodes have ellapsed since the last time the metrics relating to every episode that has ellapsed has been logged, log them again\n",
    "    if e % log_every == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close the enviornment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "mario.save()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hKYxjAfTZ2kG",
    "ntDlzPcUkFul"
   ],
   "name": "tutorial_v2.ipynb",
   "provenance": [
    {
     "file_id": "1CGyf0EV3Pm7VyPseb--M7d6IvsaeMqIE",
     "timestamp": 1603853250256
    },
    {
     "file_id": "1IdPk7OJU9QycV3fH6lt8IFMReXvKLi2B",
     "timestamp": 1603853124090
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
